{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, balanced_accuracy_score, recall_score, mutual_info_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from rmatrix.classification import RMatrixClassifier\n",
    "from decision_rules.serialization.utils import JSONSerializer\n",
    "from decision_rules.classification.ruleset import ClassificationRuleSet\n",
    "from decision_rules.measures import c2, precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_inclusion(R, M):\n",
    "  \"\"\"\n",
    "  This function takes two lists, R and M, and returns the result of the equation:\n",
    "\n",
    "  F(R) n F(M) = F(R) U F(M)\n",
    "\n",
    "  where n is the intersection and U is the union of the sets.\n",
    "\n",
    "  Args:\n",
    "    R: A list of elements.\n",
    "    M: A list of elements.\n",
    "\n",
    "  Returns:\n",
    "    A list containing the elements in the intersection and union of R and M.\n",
    "  \"\"\"\n",
    "\n",
    "  # Find the intersection of R and M using a set\n",
    "  intersection = set(R).intersection(set(M))\n",
    "\n",
    "  # Find the union of R and M using the + operator\n",
    "  union = set(R).union(set(M))\n",
    "\n",
    "  # Combine the intersection and union into a single list\n",
    "  result = len(intersection)/len(union) \n",
    "\n",
    "  return result\n",
    "\n",
    "def rmatrix_unique_rules(rule, feature_names=None):\n",
    "    rule_str = rule.premise.to_string(feature_names)\n",
    "    features = []\n",
    "    conditions = rule_str.split(\" AND \")\n",
    "    for condition in conditions:\n",
    "        feature_value = condition.split(\" = \")\n",
    "        features.append(feature_value[0])\n",
    "    \n",
    "    return list(np.unique(features))\n",
    "\n",
    "def extract_shap_fi(shap_df_row, shap_df, shap_length, stat):\n",
    "    row_values = np.abs(shap_df.iloc[shap_df_row,:].values)\n",
    "    col_names = np.array(shap_df.columns)\n",
    "    row_values_sort_idx = np.argsort(-row_values)\n",
    "    if stat == \"all\":\n",
    "      feature_top = list(col_names[row_values_sort_idx])\n",
    "    elif stat == \"max\":\n",
    "      top = np.max(shap_length)\n",
    "      feature_top = list(col_names[row_values_sort_idx])[:top]\n",
    "    elif stat == \"len\":\n",
    "      top = shap_length[shap_df_row]\n",
    "      feature_top = list(col_names[row_values_sort_idx])[:top]\n",
    "    return feature_top\n",
    "\n",
    "def precision(c) -> float:  # pylint: disable=missing-function-docstring\n",
    "    if (c.p + c.n) == 0:\n",
    "        return 0\n",
    "    return c.p / (c.p + c.n)\n",
    "\n",
    "def coverage(c) -> float:  # pylint: disable=missing-function-docstring\n",
    "    return c.p / c.P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_models = pd.read_csv(\"results/selected_bb_models.csv\")\n",
    "datasets = bb_models[\"dataset\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual inclusion - with fi global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_types = [\"_filterFF_precision_global\"]\n",
    "\n",
    "results_all = pd.DataFrame()\n",
    "\n",
    "for class_type in class_types:\n",
    "\n",
    "    for sel_dataset in tqdm(datasets):\n",
    "\n",
    "        models = np.unique(bb_models[bb_models[\"dataset\"]==sel_dataset][\"model\"])\n",
    "\n",
    "        x_train_df = pd.read_csv(f\"../results_all/{sel_dataset}/train.csv\")\n",
    "        x_train_df = x_train_df.rename(columns={'target': 'name'})\n",
    "        y_train = x_train_df[\"name\"].squeeze().astype(str)\n",
    "        x_train_df.drop(columns=[\"name\"], inplace=True)\n",
    "\n",
    "        x_test_df = pd.read_csv(f\"../results_all/{sel_dataset}/test.csv\")\n",
    "        x_test_df = x_test_df.rename(columns={'target': 'name'})\n",
    "        y_test = x_test_df[\"name\"].squeeze().astype(str)\n",
    "        x_test_df.drop(columns=[\"name\"], inplace=True)\n",
    "\n",
    "        binary_columns = list(x_train_df.columns[x_train_df.isin([0,1]).all()])\n",
    "        if len(binary_columns) > 0:\n",
    "            x_train_df[binary_columns] = x_train_df[binary_columns].astype(str)\n",
    "            x_test_df[binary_columns] = x_test_df[binary_columns].astype(str)\n",
    "\n",
    "        feature_names = x_train_df.columns\n",
    "\n",
    "        for sel_model in models:\n",
    "\n",
    "            fi_path = f\"../results_new/{sel_dataset}/{sel_model}/fi_test.csv\"\n",
    "\n",
    "            if os.path.exists(fi_path):\n",
    "\n",
    "                fi = pd.read_csv(fi_path)\n",
    "                fi_attr = fi.sort_values(by=\"importance\", ascending=False)[\"attribute\"].values\n",
    "\n",
    "                file_path= f\"../results_all/{sel_dataset}/{sel_model}/ruleset{class_type}.json\"\n",
    "                with open(file_path, 'r') as json_file:\n",
    "                    ruleset_json_read = json.load(json_file)\n",
    "\n",
    "                classifier = JSONSerializer.deserialize(ruleset_json_read, target_class=ClassificationRuleSet)\n",
    "                if \"c2\" in class_type:\n",
    "                    classifier.update(x_train_df, y_train, measure=c2)\n",
    "                elif \"precision\" in class_type:\n",
    "                    classifier.update(x_train_df, y_train, measure=precision)\n",
    "\n",
    "                rules_features = [rmatrix_unique_rules(rule, feature_names) for rule in classifier.rules]\n",
    "                rules_lengths = [len(rule) for rule in rules_features]\n",
    "                rule_max_length = np.max(rules_lengths)\n",
    "\n",
    "                fi_features_max = fi_attr[:rule_max_length]\n",
    "                fi_features_len = [fi_attr[:rule_len] for rule_len in rules_lengths]\n",
    "\n",
    "                mi = [mutual_inclusion(rules_features[obs], fi_attr) for obs in range(len(rules_features))]\n",
    "                mi_max = [mutual_inclusion(rules_features[obs], fi_features_max) for obs in range(len(rules_features))]\n",
    "                mi_len = [mutual_inclusion(rules_features[obs], fi_features_len[obs]) for obs in range(len(rules_features))]\n",
    "\n",
    "                mi_df = pd.DataFrame({'mi_all': mi, 'mi_max': mi_max, 'mi_len': mi_len})\n",
    "                mi_df[\"dataset\"] = sel_dataset\n",
    "                mi_df[\"model\"] = sel_model\n",
    "                mi_df[\"rmatrix\"] = \"c2\" if \"c2\" in class_type else \"precision\"\n",
    "\n",
    "                results_all = pd.concat([results_all, mi_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all.to_csv(\"results/mutual_inclusion_global.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual inclusion - without fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_types = [\"_filterFF_precision_approx\"]\n",
    "\n",
    "results_all = pd.DataFrame()\n",
    "\n",
    "for class_type in class_types:\n",
    "\n",
    "    for sel_dataset in tqdm(datasets):\n",
    "\n",
    "        models = np.unique(bb_models[bb_models[\"dataset\"]==sel_dataset][\"model\"])\n",
    "\n",
    "        x_train_df = pd.read_csv(f\"../results_all/{sel_dataset}/train.csv\")\n",
    "        x_train_df = x_train_df.rename(columns={'target': 'name'})\n",
    "        y_train = x_train_df[\"name\"].squeeze().astype(str)\n",
    "        x_train_df.drop(columns=[\"name\"], inplace=True)\n",
    "\n",
    "        x_test_df = pd.read_csv(f\"../results_all/{sel_dataset}/test.csv\")\n",
    "        x_test_df = x_test_df.rename(columns={'target': 'name'})\n",
    "        y_test = x_test_df[\"name\"].squeeze().astype(str)\n",
    "        x_test_df.drop(columns=[\"name\"], inplace=True)\n",
    "\n",
    "        binary_columns = list(x_train_df.columns[x_train_df.isin([0,1]).all()])\n",
    "        if len(binary_columns) > 0:\n",
    "            x_train_df[binary_columns] = x_train_df[binary_columns].astype(str)\n",
    "            x_test_df[binary_columns] = x_test_df[binary_columns].astype(str)\n",
    "\n",
    "        feature_names = x_train_df.columns\n",
    "\n",
    "        for sel_model in models:\n",
    "\n",
    "            fi_path = f\"../results_new/{sel_dataset}/{sel_model}/fi_test.csv\"\n",
    "            file_path= f\"../results_all/{sel_dataset}/{sel_model}/ruleset{class_type}.json\"\n",
    "\n",
    "            if os.path.exists(fi_path) and os.path.exists(file_path):\n",
    "\n",
    "                fi = pd.read_csv(fi_path)\n",
    "                fi_attr = fi.sort_values(by=\"importance\", ascending=False)[\"attribute\"].values\n",
    "                \n",
    "                with open(file_path, 'r') as json_file:\n",
    "                    ruleset_json_read = json.load(json_file)\n",
    "\n",
    "                classifier = JSONSerializer.deserialize(ruleset_json_read, target_class=ClassificationRuleSet)\n",
    "                if \"c2\" in class_type:\n",
    "                    classifier.update(x_train_df, y_train, measure=c2)\n",
    "                elif \"precision\" in class_type:\n",
    "                    classifier.update(x_train_df, y_train, measure=precision)\n",
    "\n",
    "                rules_features = [rmatrix_unique_rules(rule, feature_names) for rule in classifier.rules]\n",
    "                rules_lengths = [len(rule) for rule in rules_features]\n",
    "                rule_max_length = np.max(rules_lengths)\n",
    "\n",
    "                fi_features_max = fi_attr[:rule_max_length]\n",
    "                fi_features_len = [fi_attr[:rule_len] for rule_len in rules_lengths]\n",
    "\n",
    "                mi = [mutual_inclusion(rules_features[obs], fi_attr) for obs in range(len(rules_features))]\n",
    "                mi_max = [mutual_inclusion(rules_features[obs], fi_features_max) for obs in range(len(rules_features))]\n",
    "                mi_len = [mutual_inclusion(rules_features[obs], fi_features_len[obs]) for obs in range(len(rules_features))]\n",
    "\n",
    "                mi_df = pd.DataFrame({'mi_all': mi, 'mi_max': mi_max, 'mi_len': mi_len})\n",
    "                mi_df[\"dataset\"] = sel_dataset\n",
    "                mi_df[\"model\"] = sel_model\n",
    "                mi_df[\"rmatrix\"] = \"c2\" if \"c2\" in class_type else \"precision\"\n",
    "\n",
    "                results_all = pd.concat([results_all, mi_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all.to_csv(\"results/mutual_inclusion_without_fi.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai-srv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
